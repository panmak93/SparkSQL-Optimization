{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import SQLContext\n",
    "from __future__ import division\n",
    "import itertools\n",
    "import  time \n",
    "import math\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Project 6910A\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of each relation: nation, supplier, partsupp, orders, lineitem\n",
      "(25, 981, 7903, 14754, 30032)\n"
     ]
    }
   ],
   "source": [
    "#<------------------------------Experiment relations------------------------------>#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "#nation.tbl\n",
    "schema = StructType([\n",
    "    StructField(\"nationkey\", IntegerType()),\n",
    "    StructField(\"n_name\", StringType()),\n",
    "    StructField(\"regionkey\", IntegerType()),\n",
    "    StructField(\"n_comment\", StringType())\n",
    "])\n",
    "\n",
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"dbgen_database/nation.tbl\")\n",
    "df.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "#supplier.tbl\n",
    "schema2 = StructType([\n",
    "    StructField(\"suppkey\", IntegerType()),\n",
    "    StructField(\"s_name\", StringType()),\n",
    "    StructField(\"address\", StringType()),\n",
    "    StructField(\"nationkey\", IntegerType()),\n",
    "    StructField(\"phone\", StringType()),\n",
    "    StructField(\"acctbal\", DoubleType()),\n",
    "    StructField(\"s_comment\", StringType())\n",
    "])\n",
    "\n",
    "df2 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema2).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"dbgen_database/supplier.tbl\")\n",
    "df2 = df2.sample(False,0.1)\n",
    "df2.createOrReplaceTempView(\"supplier\")\n",
    "\n",
    "#partsupp.tbl\n",
    "schema3 = StructType([\n",
    "    StructField(\"partkey\", IntegerType()),\n",
    "    StructField(\"suppkey\", IntegerType()),\n",
    "    StructField(\"availqty\", IntegerType()),\n",
    "    StructField(\"supplycost\", DoubleType()),\n",
    "    StructField(\"ps_comment\", StringType())\n",
    "])\n",
    "\n",
    "df3 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema3).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"dbgen_database/partsupp.tbl\")\n",
    "df3 = df3.sample(False,0.01)\n",
    "df3.createOrReplaceTempView(\"partsupp\")\n",
    "\n",
    "#orders.tbl\n",
    "schema4 = StructType([\n",
    "    StructField(\"orderkey\", IntegerType()),\n",
    "    StructField(\"custkey\", IntegerType()),\n",
    "    StructField(\"orderstatus\", StringType()),\n",
    "    StructField(\"totalprice\", DoubleType()),\n",
    "    StructField(\"orderdate\", StringType()),\n",
    "    StructField(\"orderpriority\", StringType()),\n",
    "    StructField(\"clerk\", StringType()),\n",
    "    StructField(\"shippriority\", IntegerType()),\n",
    "    StructField(\"o_comment\", StringType())\n",
    "])\n",
    "\n",
    "df4 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema4).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"dbgen_database/orders.tbl\")\n",
    "df4 = df4.sample(False,0.01)\n",
    "df4.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "#lineitem.tbl\n",
    "schema5 = StructType([\n",
    "    StructField(\"orderkey\", IntegerType()),\n",
    "    StructField(\"partkey\", IntegerType()),\n",
    "    StructField(\"suppkey\", IntegerType()),\n",
    "    StructField(\"linenumber\", IntegerType()),\n",
    "    StructField(\"quantity\", DoubleType()),\n",
    "    StructField(\"extendedprice\", DoubleType()),\n",
    "    StructField(\"discount\", DoubleType()),\n",
    "    StructField(\"tax\", DoubleType()),\n",
    "    StructField(\"returnflag\", StringType()),\n",
    "    StructField(\"linestatus\", StringType()),\n",
    "    StructField(\"shipdate\", StringType()),\n",
    "    StructField(\"commitdate\", StringType()),\n",
    "    StructField(\"receiptdate\", StringType()),\n",
    "    StructField(\"shipinstruct\", StringType()),\n",
    "    StructField(\"shipmode\", StringType()),\n",
    "    StructField(\"l_comment\", StringType())\n",
    "    \n",
    "])\n",
    "\n",
    "df5 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema5).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"dbgen_database/lineitem.tbl\")\n",
    "df5 = df5.sample(False,0.005)\n",
    "df5.createOrReplaceTempView(\"lineitem\")\n",
    "\n",
    "print(\"Sizes of each relation: nation, supplier, partsupp, orders, lineitem\")\n",
    "print(df.count(), df2.count(), df3.count(), df4.count(), df5.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Sizes of each relation: example1_a, example1_b, example1_c\n",
      "(10, 20, 10)\n"
     ]
    }
   ],
   "source": [
    "#<--------------------------------Example relations------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "#<-----Example 1----->#\n",
    "#Relation_A.txt\n",
    "schema_eg1_1 = StructType([\n",
    "    StructField(\"courseid\", IntegerType()),\n",
    "    StructField(\"grading_scheme\", StringType())\n",
    "])\n",
    "\n",
    "dfeg1_1 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg1_1).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_1/Relation_A.txt\")\n",
    "dfeg1_1.createOrReplaceTempView(\"example1_a\")\n",
    "\n",
    "#Relation_B.txt\n",
    "schema_eg1_2 = StructType([\n",
    "    StructField(\"courseid\", IntegerType()),\n",
    "    StructField(\"professor\", StringType()),\n",
    "    StructField(\"student\", StringType())\n",
    "])\n",
    "\n",
    "dfeg1_2 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg1_2).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_1/Relation_B.txt\")\n",
    "dfeg1_2.createOrReplaceTempView(\"example1_b\")\n",
    "\n",
    "#Relation_C.txt\n",
    "schema_eg1_3 = StructType([\n",
    "    StructField(\"student\", StringType()),\n",
    "    StructField(\"club\", StringType())\n",
    "])\n",
    "\n",
    "dfeg1_3 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg1_3).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_1/Relation_C.txt\")\n",
    "dfeg1_3.createOrReplaceTempView(\"example1_c\")\n",
    "\n",
    "print(\"Example 1: Sizes of each relation: example1_a, example1_b, example1_c\")\n",
    "print(dfeg1_1.count(), dfeg1_2.count(), dfeg1_3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: Sizes of each relation: example2_a, example2_b, example2_c\n",
      "(18, 7, 8)\n"
     ]
    }
   ],
   "source": [
    "#<--------------------------------Example relations------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "#<-----Example 2----->#\n",
    "#Relation_A.txt\n",
    "schema_eg2_1 = StructType([\n",
    "    StructField(\"personid\", IntegerType()),\n",
    "    StructField(\"sport\", StringType())\n",
    "])\n",
    "\n",
    "dfeg2_1 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg2_1).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_2/Relation_A.txt\")\n",
    "dfeg2_1.createOrReplaceTempView(\"example2_a\")\n",
    "\n",
    "#Relation_B.txt\n",
    "schema_eg2_2 = StructType([\n",
    "    StructField(\"personid\", IntegerType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"gender\", StringType())\n",
    "])\n",
    "\n",
    "dfeg2_2 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg2_2).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_2/Relation_B.txt\")\n",
    "dfeg2_2.createOrReplaceTempView(\"example2_b\")\n",
    "\n",
    "#Relation_C.txt\n",
    "schema_eg2_3 = StructType([\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"cities\", StringType())\n",
    "])\n",
    "\n",
    "dfeg2_3 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg2_3).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_2/Relation_C.txt\")\n",
    "dfeg2_3.createOrReplaceTempView(\"example2_c\")\n",
    "\n",
    "print(\"Example 2: Sizes of each relation: example2_a, example2_b, example2_c\")\n",
    "print(dfeg2_1.count(), dfeg2_2.count(), dfeg2_3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Sizes of each relation: example3_a, example3_b, example3_c\n",
      "(25, 5, 50)\n"
     ]
    }
   ],
   "source": [
    "#<--------------------------------Example relations------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "#<-----Example 3----->#\n",
    "#Relation_A.txt\n",
    "schema_eg3_1 = StructType([\n",
    "    StructField(\"shape\", StringType()),\n",
    "    StructField(\"colour\", StringType()),\n",
    "    StructField(\"transparency\", StringType())\n",
    "])\n",
    "\n",
    "dfeg3_1 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg3_1).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_3/Relation_A.txt\")\n",
    "dfeg3_1.createOrReplaceTempView(\"example3_a\")\n",
    "\n",
    "#Relation_B.txt\n",
    "schema_eg3_2 = StructType([\n",
    "    StructField(\"shape\", IntegerType()),\n",
    "    StructField(\"age\", IntegerType())\n",
    "])\n",
    "\n",
    "dfeg3_2 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg3_2).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_3/Relation_B.txt\")\n",
    "dfeg3_2.createOrReplaceTempView(\"example3_b\")\n",
    "\n",
    "#Relation_C.txt\n",
    "schema_eg3_3 = StructType([\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"names\", StringType())\n",
    "])\n",
    "\n",
    "dfeg3_3 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg3_3).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_3/Relation_C.txt\")\n",
    "dfeg3_3.createOrReplaceTempView(\"example3_c\")\n",
    "\n",
    "print(\"Example 3: Sizes of each relation: example3_a, example3_b, example3_c\")\n",
    "print(dfeg3_1.count(), dfeg3_2.count(), dfeg3_3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: Sizes of each relation:  example4_a, example4_b, example4_c\n",
      "(26, 6, 0)\n"
     ]
    }
   ],
   "source": [
    "#<--------------------------------Example relations------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "#<-----Example 4----->#\n",
    "#Relation_A.txt\n",
    "schema_eg4_1 = StructType([\n",
    "    StructField(\"names\", StringType()),\n",
    "    StructField(\"birthyear\", IntegerType())\n",
    "])\n",
    "\n",
    "dfeg4_1 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg4_1).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_4/Relation_A.txt\")\n",
    "dfeg4_1.createOrReplaceTempView(\"example4_a\")\n",
    "\n",
    "#Relation_B.txt\n",
    "schema_eg4_2 = StructType([\n",
    "    StructField(\"grading\", IntegerType()),\n",
    "    StructField(\"movienames\", StringType())\n",
    "])\n",
    "\n",
    "dfeg4_2 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg4_2).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_4/Relation_B.txt\")\n",
    "dfeg4_2.createOrReplaceTempView(\"example4_b\")\n",
    "\n",
    "#Relation_C.txt\n",
    "schema_eg4_3 = StructType([\n",
    "])\n",
    "\n",
    "dfeg4_3 = sqlContext.read.format(\"com.databricks.spark.csv\").schema(schema_eg4_3).option(\"mode\", \"PERMISSIVE\").option('delimiter', '|').load(\"Example_data/Example_4/Relation_C.txt\")\n",
    "dfeg4_3.createOrReplaceTempView(\"example4_c\")\n",
    "\n",
    "print(\"Example 4: Sizes of each relation:  example4_a, example4_b, example4_c\")\n",
    "print(dfeg4_1.count(), dfeg4_2.count(), dfeg4_3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<-----------------------------------Optimizer------------------------------------>#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "def join_size_equality(df_A, df_B, attributes_list):\n",
    "    #Actual computation of join sizes\n",
    "    card_A = df_A.count()\n",
    "    card_B = df_B.count()\n",
    "    card_attributes = list()\n",
    "    num = card_A * card_B\n",
    "    denom = 1\n",
    "    if len(attributes_list) == 0:\n",
    "        return num\n",
    "    else:\n",
    "        for attribute in attributes_list:\n",
    "            card_attributes += [max(count_Distinct(df_A, attribute), count_Distinct(df_B, attribute))]\n",
    "        for card_attribute in card_attributes: \n",
    "            denom *= card_attribute\n",
    "        return math.ceil(num/denom)\n",
    "    \n",
    "\n",
    "def count_Distinct(df, attribute):\n",
    "    #Self implemented count distinct function, for spark's implementation slower\n",
    "    count_dict = dict()\n",
    "    count = 0\n",
    "    for element in df.collect():\n",
    "        element_tuple = str(element[attribute])\n",
    "        if element_tuple not in count_dict:   \n",
    "            count_dict[element_tuple] = 1\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "def matching_attributes(df_A, df_B):\n",
    "    #returns list of common attributes\n",
    "    #Natural Join\n",
    "    attributes_list = list()\n",
    "    for fielda in df_A.schema:\n",
    "        for fieldb in df_B.schema:\n",
    "            if fielda.name == fieldb.name:\n",
    "                attributes_list += [fielda.name]\n",
    "    return attributes_list\n",
    "\n",
    "def query_optimizer(relation_pairs, relations_list):\n",
    "    #actual query_optimizer put together\n",
    "    #Three relation case\n",
    "    performance_dict = dict()\n",
    "    df_dict = dict()\n",
    "    for relation in relations_list:\n",
    "        df_dict[relation] = spark.table(relation)\n",
    "    for relation_pair in relation_pairs:\n",
    "        attributes_list = matching_attributes(df_dict[relation_pair[0]], df_dict[relation_pair[1]])\n",
    "        performance = join_size_equality(df_dict[relation_pair[0]], df_dict[relation_pair[1]], attributes_list)\n",
    "        print(relation_pair[0] + \" + \" + relation_pair[1] + \" Join Size: \" + str(performance))\n",
    "        performance_dict[relation_pair] = performance\n",
    "    shortest = 99999 ^ 9\n",
    "    optimal_order = tuple()\n",
    "    for relation_pair in relation_pairs:\n",
    "        if shortest > performance_dict[relation_pair]:\n",
    "            shortest = performance_dict[relation_pair]\n",
    "            optimal_order = relation_pair\n",
    "    return shortest, optimal_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your SQL query here:SELECT * FROM nation JOIN supplier JOIN partsupp \n",
      "nation + supplier Join Size: 981.0\n",
      "nation + partsupp Join Size: 197575\n",
      "supplier + partsupp Join Size: 1423.0\n",
      "Optimal: Join nation and supplier first with join size: 981.0\n"
     ]
    }
   ],
   "source": [
    "#<---------------------------------Query Digest----------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "def input_query():\n",
    "    #return \"SELECT * FROM supplier JOIN orders JOIN lineitem\"\n",
    "    return str(raw_input(\"Enter your SQL query here:\"))\n",
    "\n",
    "def query_digest(query):\n",
    "    #assumes query format valid\n",
    "    query = query.lower()\n",
    "    string_list = query.split()\n",
    "    from_index = string_list.index(\"from\")\n",
    "    intervals = [i for i, a in enumerate(string_list) if a == \"join\"] + [len(string_list)]\n",
    "    join_intervals = list()\n",
    "    for i in range(len(intervals)-1):\n",
    "        join_intervals += [(intervals[i], intervals[i+1])]\n",
    "    \n",
    "    relations = list()\n",
    "    conditions = dict()\n",
    "    relations += [string_list[from_index+1]]\n",
    "    j = 0\n",
    "    for index in join_intervals:\n",
    "        relations += [string_list[index[0]+1]]\n",
    "        #Conditions omitted (Note: Algorithm automatically checks for matching attribute names)\n",
    "        '''join = string_list[index[0]:index[1]+1]\n",
    "        if \"on\" in join:\n",
    "            on_index = join.index(\"on\")\n",
    "            and_indices = [on_index] + [i for i, a in enumerate(join) if a == \"and\"]\n",
    "            for index2 in and_indices:\n",
    "                if j not in conditions:\n",
    "                    conditions[j] = list()\n",
    "                else:\n",
    "                    conditions[j] += [join[index2+1] + join[index2+2] +join[index2+3]]\n",
    "        else:\n",
    "            conditions[j] = \"\"\n",
    "            '''\n",
    "    return relations\n",
    "\n",
    "query_relations = query_digest(input_query())\n",
    "query_combinations = list(itertools.combinations(query_relations, 2))\n",
    "do_first = query_optimizer(query_combinations, query_relations)\n",
    "print(\"Optimal: Join \" + do_first[1][0] + \" and \" + do_first[1][1] +\" first with join size: \" + str(do_first[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<-----------------Comparing actual join runtimes (in seconds)-------------------->#\n",
    "#<-----------------------------Not part of Optimizer------------------------------>#\n",
    "#--------------------------------------------------------------------------------->#\n",
    "\n",
    "#s = time.time()\n",
    "#df.join(df2.join(df3,['suppkey']),['nationkey']).show()\n",
    "#e = time.time ()\n",
    "#df3.join(df.join(df2,['nationkey']),['suppkey']).show()\n",
    "#e2 = time.time()\n",
    "#print(e-s,e2-e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
